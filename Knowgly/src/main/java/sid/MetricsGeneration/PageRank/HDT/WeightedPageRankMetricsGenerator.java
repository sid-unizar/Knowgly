package sid.MetricsGeneration.PageRank.HDT;

import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.jena.query.QuerySolution;
import org.apache.jena.query.ResultSet;
import org.apache.jena.rdf.model.Literal;
import org.apache.jena.rdf.model.Resource;
import org.rdfhdt.hdt.enums.TripleComponentRole;
import org.rdfhdt.hdt.exceptions.ParserException;
import org.rdfhdt.hdt.hdt.HDT;
import org.rdfhdt.hdt.hdt.HDTManager;
import org.rdfhdt.hdt.triples.IteratorTripleID;
import org.rdfhdt.hdt.triples.TripleID;
import sid.MetricsGeneration.PageRank.SPARQL.PageRankMetricsGenerator;
import sid.MetricsGeneration.SPARQL.InfoRankMetricsGenerator;
import sid.MetricsGeneration.util.BigDoubleArray;
import sid.MetricsGeneration.util.BigIntArray;
import sid.MetricsGeneration.util.BinarySearch;
import sid.SPARQLEndpoint.LocalHDTSPARQLEndpoint;
import sid.SPARQLEndpoint.SPARQLEndpoint;
import sid.utils.Pair;

import java.io.FileNotFoundException;
import java.io.IOException;
import java.io.PrintWriter;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutionException;
import java.util.concurrent.ForkJoinPool;
import java.util.stream.LongStream;

/**
 * PageRankMetricsGenerator specialization for an HDT model, adapted from
 * --TEMPORARILY REDACTED FOR ANONYMIZATION PURPOSES--
 * <p>
 * Bidirectional and weighted with InfoRank's W(r,p) metric, as specified in:
 * Menendez, Elisa & Casanova, Marco & Paes Leme, Luiz Andr√© & Boughanem, Mohand. (2019).
 * Novel Node Importance Measures to Improve Keyword Search over RDF Graphs.
 * 10.1007/978-3-030-27618-8_11.
 */
public class WeightedPageRankMetricsGenerator extends PageRankMetricsGenerator {
    private static final String CONFIGURATION_FILE = "configuration/metricsConfiguration.json";
    public static final String METRICS_HDT_FILE_CONF = "metricsToHDTFile";

    public static final String W_SPARQL = "configuration/queries/metrics_aggregator_queries/w.sparql";
    public static final String ALL_W_SPARQL = "configuration/queries/metrics_aggregator_queries/all_w.sparql";

    // HDT file containing the original KG
    private HDT KGHDT;

    // HDT file containing, as a minimum, the contents of the infoRank subgraph generated by InfoRankMetricsGenerator
    // (This should have been generated automatically by it)
    private HDT metricsHDT;
    private BigIntArray numberOutgoing;
    // we use this one to store the reference and do the swap
    private BigDoubleArray pageRankScoresShared;
    private BigDoubleArray pageRankScoresPrev;
    private BigDoubleArray pageRankScoresNext;
    private BigDoubleArray pageRankScoresObjects;

    // Used to identify all literals, since the entries in the dictionary are ordered, an interval suffices
    private long start_literals_objects = -1;
    private long end_literals_objects = -1;

    String wQuery;
    String allWQuery;

    private final SPARQLEndpoint endpoint; // Endpoint to which we will ask the InfoRank w scores for <s,p> pairs

    // Whether to calculate W(r,p) online or offline (storing it in the KG and querying it afterwards).
    // The latter consumes a lot of space and increases the Weighted PageRank calculation times a lot,
    // so it's not recommended.
    //
    // For now, this is always true, as calculating it as a metric in the KG has been disabled
    private final boolean w_online = true;

    private final long irPredicateID;

    // Concurrent map of (r,p) -> W(r,p))), serving as a cache from the second iteration onwards
    ConcurrentHashMap<Pair<Long, Long>, Double> wRPCache;

    // Concurrent map of p -> IR(p), serving as a cache from the second iteration onwards
    // Gives a decent speedup
    ConcurrentHashMap<Long, Double> irPCache;

    public static WeightedPageRankMetricsGenerator fromConfigurationFile(SPARQLEndpoint endpoint) throws IOException {
        byte[] mapData = Files.readAllBytes(Paths.get(CONFIGURATION_FILE));

        ObjectMapper objectMapper = new ObjectMapper();
        JsonNode rootNode = objectMapper.readTree(mapData);

        return new WeightedPageRankMetricsGenerator(
                rootNode.get(INPUT_FILE_CONF).asText(),
                rootNode.get(METRICS_HDT_FILE_CONF).asText(),
                rootNode.get(OUTPUT_FILE_HDT_CONF).asText(),
                rootNode.get(DAMPING_FACTOR_CONF).asDouble(),
                rootNode.get(START_VALUE_CONF).asDouble(),
                rootNode.get(NUMBER_OF_ITERATIONS_CONF).asInt(),
                rootNode.get(CONSIDER_LITERALS_CONF).asBoolean(),
                rootNode.get(PARALLELIZE_CONF).asBoolean(),
                rootNode.get(OUTPUT_FILE_CONF).asText(),
                endpoint
        );
    }

    // Constructor with default PageRank parameters
    public WeightedPageRankMetricsGenerator(String KGHDTFile,
                                            String metricsHDTFile,
                                            String RDFOutputFile,
                                            String HDTOutputFile,
                                            SPARQLEndpoint endpoint) throws IOException {
        super(RDFOutputFile, HDTOutputFile);

        System.out.println("Loading HDT file for PageRank (if it wasn't indexed yet it may take a few minutes)...");
        this.loadMainHDT(KGHDTFile);
        this.endpoint = endpoint;

        loadMetricsHDT(metricsHDTFile);
        irPredicateID = metricsHDT.getDictionary().stringToId(
                // Or IR_P_URI, they are the same
                InfoRankMetricsGenerator.IR_D_URI,
                TripleComponentRole.PREDICATE);
        wRPCache = new ConcurrentHashMap<>();
        irPCache = new ConcurrentHashMap<>();

        Path factFrequencyQueryPath = Path.of(W_SPARQL);
        this.wQuery = Files.readString(factFrequencyQueryPath);

        Path allFactFrequencyQueryPath = Path.of(ALL_W_SPARQL);
        this.allWQuery = Files.readString(allFactFrequencyQueryPath);
    }

    // Constructor with custom PageRank parameters
    public WeightedPageRankMetricsGenerator(String KGHDTFile,
                                            String metricsHDTFile,
                                            String HDTOutputFile,
                                            double dampingFactor,
                                            double startValue,
                                            int numberOfIterations,
                                            boolean considerLiterals,
                                            boolean parallelize,
                                            String RDFOutputFile,
                                            SPARQLEndpoint endpoint) throws IOException {
        super(RDFOutputFile,
                HDTOutputFile,
                dampingFactor,
                startValue,
                numberOfIterations,
                considerLiterals,
                parallelize);

        System.out.println("Loading HDT file for PageRank (if it wasn't indexed yet it may take a few minutes)...");
        this.loadMainHDT(KGHDTFile);
        this.endpoint = endpoint;

        loadMetricsHDT(metricsHDTFile);
        irPredicateID = metricsHDT.getDictionary().stringToId(
                // Or IR_P_URI, they are the same
                InfoRankMetricsGenerator.IR_D_URI,
                TripleComponentRole.PREDICATE);
        wRPCache = new ConcurrentHashMap<>();
        irPCache = new ConcurrentHashMap<>();

        Path factFrequencyQueryPath = Path.of(W_SPARQL);
        this.wQuery = Files.readString(factFrequencyQueryPath);

        Path allFactFrequencyQueryPath = Path.of(ALL_W_SPARQL);
        this.allWQuery = Files.readString(allFactFrequencyQueryPath);
    }

    private void loadMainHDT(String hdtFile) {
        try {
            KGHDT = HDTManager.mapIndexedHDT(hdtFile, null);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    private void loadMetricsHDT(String hdtFile) {
        try {
            metricsHDT = HDTManager.mapIndexedHDT(hdtFile, null);
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    @Override
    public void compute() throws ExecutionException, InterruptedException {
        System.out.println("Computing PageRank: " + numberOfIterations +
                " iterations, damping factor " + dampingFactor +
                ", start value " + startValue +
                ", considering literals " + considerLiterals);

        long nShared = KGHDT.getDictionary().getNshared();
        long nSubjects = KGHDT.getDictionary().getNsubjects();
        long nObjects = KGHDT.getDictionary().getNobjects();

        // 0 <= nShared < nSubjects
        // 0 <= nObjects (in the same dictionary as shared and subjects, may have collisions)
        // 0 <= nPredicates (Separate dictionary, no collisions)
        //
        // nShared are subjects which appear on both sides, and share the same range as subjects and objects
        //System.out.println("nShared: " + nShared);
        //System.out.println("nSubjects: " + nSubjects);
        //System.out.println("nObjects: " + nObjects);


        long numberNonLiterals = 0;
        start_literals_objects = -1;
        end_literals_objects = -1;
        if (!considerLiterals) {
            start_literals_objects = BinarySearch.first(KGHDT.getDictionary(), nShared + 1, nObjects, "\"");
            end_literals_objects = BinarySearch.last(KGHDT.getDictionary(), nShared + 1, nObjects, nObjects, "\"");
        }

        numberNonLiterals = nObjects - (end_literals_objects - start_literals_objects);

        // We only have to store the prev and next values of the shared elements,
        // the rest of elements are just sinks
        pageRankScoresPrev = new BigDoubleArray((int) KGHDT.getDictionary().getNsubjects() + 1);
        pageRankScoresNext = new BigDoubleArray((int) KGHDT.getDictionary().getNsubjects() + 1);
        pageRankScoresObjects = new BigDoubleArray(numberNonLiterals + 1);

        // Initialize the start page rank scores (only the prev set)
        for (int id = 1; id <= nShared; id++) {
            pageRankScoresPrev.set(id, startValue);
        }
        for (int k = 0; k < numberNonLiterals; k++) {
            pageRankScoresObjects.set(k, startValue);
        }

        // Compute the number of outgoing links
        numberOutgoing = new BigIntArray(KGHDT.getDictionary().getNsubjects() + 1);
        numberOutgoing.set(0, 0);
        for (long id = 1; id <= nSubjects; id++) {
            IteratorTripleID iteratorTripleID = KGHDT.getTriples().search(new TripleID(id, 0, 0));
            int count = 0;
            while (iteratorTripleID.hasNext()) {
                long o = iteratorTripleID.next().getObject();

                if (o <= nSubjects) {
                    count++;
                }
            }
            numberOutgoing.set(id, count);
        }

        System.out.println("Iterations:");

        for (int j = 0; j < numberOfIterations; j++) {
            System.out.print(j + " ");
            // CBL: Having separated the read and write access assures us that pageRankScoresSharedPrev is
            // accessed only in readOnly mode, and in fact, the set position always affects different
            // elements. The only "problematic" call could be HDT binary search, but I assume that it's a read
            // only operation, with no side effect at all
            LongStream stream;

            if (parallelize) {
                stream = LongStream.range(1, nSubjects + 1).parallel();
            } else {
                stream = LongStream.range(1, nSubjects + 1);
            }

            ForkJoinPool workers = new ForkJoinPool(Runtime.getRuntime().availableProcessors());
            workers.submit(() ->
                    stream.forEach(id -> {
                        IteratorTripleID outgoingLinks = KGHDT.getTriples().search(new TripleID(id, 0, 0));
                        IteratorTripleID incomingLinks = KGHDT.getTriples().search(new TripleID(0, 0, id));

                        // Map of non-duplicate <subject, predicate> IDs seen in links from both directions
                        Map<Long, Long> subjectPreds = new HashMap<>();
                        while (outgoingLinks.hasNext()) {
                            TripleID nextLink = outgoingLinks.next();

                            long o = nextLink.getObject();
                            long p = nextLink.getPredicate();

                            // We only want objects which are valid URIs AND appear as subjects somewhere else in the KG.
                            // Since it is appearing as an object, it must be in the (0, nShared) range.
                            if (o <= nShared) {
                                subjectPreds.put(o, p);
                            }
                        }

                        while (incomingLinks.hasNext()) {
                            TripleID nextLink = incomingLinks.next();

                            long s = nextLink.getSubject();
                            long p = nextLink.getPredicate();

                            // Let it be known that, in 2023-03-20, I wondered for 3 hours why HDT was throwing incoherent
                            // triples at me, since I was doing this...
                            //long s = incomingLinks.next().getSubject();
                            //long p = incomingLinks.next().getPredicate();
                            //long o = incomingLinks.next().getObject();

                            subjectPreds.put(s, p);
                        }

                        double pageRank = (1.0D - dampingFactor);
                        for (var entry : subjectPreds.entrySet()) {
                            // subject in <id, p, s> or <s, p, id>, where id is the entity whose PR we are calculating
                            long s = entry.getKey();
                            long p = entry.getValue();

                            double pageRankIn;

                            pageRankIn = pageRankScoresPrev.get(s);

                            int numberOut = numberOutgoing.get(s); // Outgoing links from s

                            if (numberOut != 0) { // Avoid infinites
                                // Now we also add PageRank's weight (for the entity id!)
                                pageRank += dampingFactor * (pageRankIn / numberOut) * getWeight(id, p);
                            }
                        }

                        pageRankScoresNext.set(id, pageRank);
                    })
            ).get();

            workers.close();

            // CBL: we swap Prev and Next storages
            // I take advantage of the already existing variable to
            // make the reference swap and reuse the memory
            pageRankScoresShared = pageRankScoresNext;
            pageRankScoresNext = pageRankScoresPrev;
            pageRankScoresPrev = pageRankScoresShared;

        }
        //  CBL: in the last iteration pageRankScoresShared == pageRankScoresSharedNext
        System.out.println("\n");
    }

    private double getWeight(long r, long p) {
        if (w_online) { // For now, this is always true
            return getWeightFromHDT(r, p);
        } else {
            return getWeightFromSPARQL(r, p);
        }
    }

    private double getWeightFromHDT(long r, long p) {
        double wFromCache = checkWRPCache(r, p);

        if (wFromCache != -1.0) {
            return wFromCache;
        }

        double irP = getIRP(p);

        // Keep a set of all the predicates we may find so that we don't take them into account more than once in the
        // steps below
        Set<Long> predicatesFound = new HashSet<>();

        // Sum of IR(q), where r acts as a subject
        double sumIRQSubject = 0.0;
        IteratorTripleID irQIterator = KGHDT.getTriples().search(new TripleID(r, 0, 0));
        while (irQIterator.hasNext()) {
            TripleID tripleID = irQIterator.next();
            long irQPred = tripleID.getPredicate();


            if (!predicatesFound.contains(irQPred)) {
                sumIRQSubject += getIRP(irQPred);
                predicatesFound.add(irQPred);
            }
        }

        // Sum of IR(q), where r acts as an object
        double sumIRQObject = 0.0;
        irQIterator = KGHDT.getTriples().search(new TripleID(0, 0, r));
        while (irQIterator.hasNext()) {
            TripleID tripleID = irQIterator.next();
            long irQPred = tripleID.getPredicate();

            // Only add it if we didn't find it when querying with r as a subject
            if (!predicatesFound.contains(irQPred)) {
                sumIRQObject += getIRP(irQPred);
                predicatesFound.add(irQPred);
            }
        }

        double w = irP / (sumIRQSubject + sumIRQObject);
        addToWRPCache(r, p, w);
        return w;
    }


    // Caching helper functions

    double checkWRPCache(long r, long p) {
        return wRPCache.getOrDefault(new Pair<>(r, p), -1.0);
    }

    void addToWRPCache(long r, long p, double w) {
        wRPCache.put(new Pair<>(r, p), w);
    }

    double checkIrPCache(long p) {
        return irPCache.getOrDefault(p, -1.0);
    }

    void addToIrPCache(long p, double ir) {
        irPCache.put(p, ir);
    }


    // Given a predicate ID in KGHDT, return its IR(P) contained in metricsHDT (intermediate conversions are
    // handled internally).
    //
    // If it doesn't exist, it will return 0.0
    private double getIRP(long p) {
        double irPCache = checkIrPCache(p);
        if (irPCache != -1.0) {
            return irPCache;
        }

        // Get the predicate's string in the KG's HDT, and then search for its ID in the metrics HDT, since they are not
        // the same ones
        String predicateInKGString = KGHDT.getDictionary().idToString(p, TripleComponentRole.PREDICATE).toString();
        long predicateIDInMetrics = metricsHDT.getDictionary().stringToId(predicateInKGString, TripleComponentRole.SUBJECT); // It acts as a subject here!

        double irP;
        IteratorTripleID irPIterator = metricsHDT.getTriples().search(new TripleID(predicateIDInMetrics, irPredicateID, 0));
        if (irPIterator.hasNext()) {
            TripleID tripleID = irPIterator.next();
            long irPID = tripleID.getObject();
            CharSequence literalString = metricsHDT.getDictionary().getObjects().extract(irPID);
            // Do it manually. Creating an intermediate Jena literal, for some reason, will throw numeric format
            // exceptions when unboxing it again, and it's also costly
            String splitLiteralString = literalString.toString().split("\\^")[0];
            if (splitLiteralString.charAt(0) == '"')
                splitLiteralString = splitLiteralString.substring(1, splitLiteralString.length() - 1); // Remove surrounding '"'s
            irP = Double.parseDouble(splitLiteralString);

            addToIrPCache(p, irP);
            return irP;
        } else {
            // This only happens when we attempt to query the IR of a predicate which only appears
            // in triples whose object is not a literal (rdf:type...)
            addToIrPCache(p, 0.0);
            return 0.0;
        }
    }

    private double getWeightFromSPARQL(long r, long p) {
        String wURI = "<" + KGHDT.getDictionary().idToString(r, TripleComponentRole.SUBJECT).toString() +
                KGHDT.getDictionary().idToString(p, TripleComponentRole.PREDICATE).toString() + ">";

        ResultSet rs = endpoint.runSelectQuery(wQuery.formatted(wURI));

        if (rs.hasNext()) {
            QuerySolution qs = rs.next();

            Literal w = qs.getLiteral("w");

            return w.getDouble();
        } else {
            System.err.println("Couldn't find the W(r,p) score for " + wURI + " during PageRank calculation!");
            return 0.0;
        }
    }

    // Unused, this will cause OOMs for big KGs
    private HashMap<String, Double> getAllWeightsFromSPARQL() {
        HashMap<String, Double> res = new HashMap<>();
        ResultSet rs = endpoint.runSelectQuery(allWQuery);

        while (rs.hasNext()) {
            QuerySolution qs = rs.next();

            Resource s = qs.getResource("s");
            Literal w = qs.getLiteral("w");

            res.put(s.getURI(), w.getDouble());
        }

        return res;
    }

    @Override
    public void writePageRankScoresAsNtriples() throws FileNotFoundException {
        System.out.println("Writing PageRank results as RDF...");
        PrintWriter writer = new PrintWriter(RDFOutputFile);

        long nShared = KGHDT.getDictionary().getNshared();
        long nSubjects = KGHDT.getDictionary().getNsubjects();
        long nObjects = KGHDT.getDictionary().getNobjects();

        for (long id = 1; id <= nSubjects; id++) {
            writer.println("<" + sanitizeURI(KGHDT.getDictionary().idToString(id, TripleComponentRole.SUBJECT).toString()) + "> <http://purl.org/voc/vrank#pagerank>\t \"" + String.format(Locale.US, "%.10f", pageRankScoresShared.get(id)) + "\"^^<http://www.w3.org/2001/XMLSchema#float> .");
        }

        for (long id = nShared + 1; id <= nObjects; id++) {
            if (id < start_literals_objects) {
                writer.println("<" + sanitizeURI(KGHDT.getDictionary().idToString(id, TripleComponentRole.OBJECT).toString()) + "> <http://purl.org/voc/vrank#pagerank>\t \"" + String.format(Locale.US, "%.10f", pageRankScoresObjects.get(id)) + "\"^^<http://www.w3.org/2001/XMLSchema#float> .");
            }

            if (id > end_literals_objects) {
                writer.println("<" + sanitizeURI(KGHDT.getDictionary().idToString(id, TripleComponentRole.OBJECT).toString()) + "> <http://purl.org/voc/vrank#pagerank>\t \"" + String.format(Locale.US, "%.10f", pageRankScoresObjects.get(id - (end_literals_objects - start_literals_objects))) + "\"^^<http://www.w3.org/2001/XMLSchema#float> .");
            }
        }

        writer.flush();
    }

    @Override
    public LocalHDTSPARQLEndpoint writePageRankScoresHDT() throws IOException, ParserException {
        // Could be done directly, but it's not costly in time for what we do everywhere else
        writePageRankScoresAsNtriples();

        // If the number of subjects is big enough (more than in the 2016-10 dump of the DBpedia),
        // normal HDT constructors will not work with the RDF file, so we use catTree as an alternative
        return LocalHDTSPARQLEndpoint.fromRDFCatTree(RDFOutputFile,
                HDTOutputFile,
                "unused", // All URIs are well-formed and no more data is going to be inserted
                "ntriples",
                true,
                false); // Delete the input file too
    }
}
